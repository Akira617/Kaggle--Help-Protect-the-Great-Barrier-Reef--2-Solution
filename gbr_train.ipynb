{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🛠 Install Libraries","metadata":{"papermill":{"duration":0.040676,"end_time":"2022-01-18T14:07:16.821778","exception":false,"start_time":"2022-01-18T14:07:16.781102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -qU bbox-utility # check https://github.com/awsaf49/bbox for source code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":17.807711,"end_time":"2022-01-18T14:07:34.671184","exception":false,"start_time":"2022-01-18T14:07:16.863473","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📚 Import Libraries","metadata":{"papermill":{"duration":0.041418,"end_time":"2022-01-18T14:07:34.754905","exception":false,"start_time":"2022-01-18T14:07:34.713487","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.30836,"end_time":"2022-01-18T14:07:35.106006","exception":false,"start_time":"2022-01-18T14:07:34.797646","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLD      = 1 # which fold to train\nDIM       = 3100 # image size\nMODEL     = 'yolov5s6' # model\nBATCH     = 2 # batch size\nEPOCHS    = 20\nOPTMIZER  = 'Adam' # optimizer\n\nPROJECT   = 'great-barrier-reef-public' # project name\nNAME      = f'{MODEL}-dim{DIM}-fold{FOLD}' # experiment name\n\nREMOVE_NOBBOX = True # remove images with no bbox \nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/' # data dir \nIMAGE_DIR = '/kaggle/images' # directory to save images\nLABEL_DIR = '/kaggle/labels' # directory to save labels","metadata":{"papermill":{"duration":0.050928,"end_time":"2022-01-18T14:07:38.620624","exception":false,"start_time":"2022-01-18T14:07:38.569696","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Directories","metadata":{"papermill":{"duration":0.04245,"end_time":"2022-01-18T14:07:38.705542","exception":false,"start_time":"2022-01-18T14:07:38.663092","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","metadata":{"papermill":{"duration":1.346658,"end_time":"2022-01-18T14:07:40.094401","exception":false,"start_time":"2022-01-18T14:07:38.747743","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Paths","metadata":{"papermill":{"duration":0.050363,"end_time":"2022-01-18T14:07:40.189320","exception":false,"start_time":"2022-01-18T14:07:40.138957","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg' # 数据原始路径\ndf['image_path']  = f'{IMAGE_DIR}/'+df.image_id+'.jpg' # yolov5 读取images的路径\ndf['label_path']  = f'{LABEL_DIR}/'+df.image_id+'.txt' # yolov5 读取labels的路径\ndf['annotations'] = df['annotations'].progress_apply(eval) # 将标注从str 转换成 list\ndisplay(df.head(2))\n\n# 统计No BBox和With BBox数量\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"papermill":{"duration":0.490128,"end_time":"2022-01-18T14:07:40.725291","exception":false,"start_time":"2022-01-18T14:07:40.235163","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In this notebook, we use only bboxed-images (~5k). We can use all ~23K images for train but most of them don't have any labels.\n# So it would be easier to carry out experiments using only bboxed images.\nif REMOVE_NOBBOX:\n    # 只挑选出有boxes的图像\n    df = df.query(\"num_bbox>0\")","metadata":{"papermill":{"duration":0.064426,"end_time":"2022-01-18T14:07:41.191407","exception":false,"start_time":"2022-01-18T14:07:41.126981","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to copy the Images to Current Directory(`/kaggle/working`) \n# as `/kaggle/input` doesn't have write access which is needed for YOLOv5.\n# We can make this process faster using Joblib which uses Parallel computing.\n\ndef make_copy(row):\n    shutil.copyfile(row.old_image_path, row.image_path)\n    return\n\nimage_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))","metadata":{"papermill":{"duration":0.050398,"end_time":"2022-01-18T14:07:41.373881","exception":false,"start_time":"2022-01-18T14:07:41.323483","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🔨 Helper","metadata":{"papermill":{"duration":0.046665,"end_time":"2022-01-18T14:08:11.271568","exception":false,"start_time":"2022-01-18T14:08:11.224903","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\ndef get_bbox(annots):\n    '''\n    get bboxes list\n    '''\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    '''\n    get image size\n    '''\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(42)","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.882451,"end_time":"2022-01-18T14:08:12.203589","exception":false,"start_time":"2022-01-18T14:08:11.321138","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create BBox","metadata":{"papermill":{"duration":0.046668,"end_time":"2022-01-18T14:08:12.297909","exception":false,"start_time":"2022-01-18T14:08:12.251241","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)\n\ndf['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","metadata":{"papermill":{"duration":0.115804,"end_time":"2022-01-18T14:08:12.460998","exception":false,"start_time":"2022-01-18T14:08:12.345194","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🏷️ Create Labels","metadata":{"papermill":{"duration":0.049483,"end_time":"2022-01-18T14:08:12.775891","exception":false,"start_time":"2022-01-18T14:08:12.726408","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cnt = 0\nall_bboxes = []\nbboxes_info = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height # 图片高度\n    image_width  = row.width # 图片宽度\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy() # 目标框列表\n    num_bbox     = len(bboxes_coco) # 目标框数量\n    labels       = np.array([0]*num_bbox)[..., None].astype(str) # 类别（因为本次项目只有一种类别，所以所有值都是0）\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            # 如果没有bboxes，直接返回''\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        # bboxes 转成yolo所需的格式\n        bboxes_voc  = coco2voc(bboxes_coco, image_height, image_width)\n        bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n        bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n        # 存下bboxes和其相关信息\n        all_bboxes.extend(bboxes_yolo.astype(float))\n        bboxes_info.extend([[row.image_id, row.video_id, row.sequence]]*len(bboxes_yolo))\n        # write annots\n        annots = np.concatenate([labels, bboxes_yolo], axis=1)\n        string = annot2str(annots) \n        f.write(string) # 类似 \"0 0.47890624 0.27569443 0.0234375 0.037499994\"\nprint('Missing:',cnt)","metadata":{"_kg_hide-input":false,"papermill":{"duration":4.692142,"end_time":"2022-01-18T14:08:17.517103","exception":false,"start_time":"2022-01-18T14:08:12.824961","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📁 Create Folds\n> Number of samples aren't same in each fold which can create large variance in **Cross-Validation**.","metadata":{"papermill":{"duration":0.052238,"end_time":"2022-01-18T14:08:17.624710","exception":false,"start_time":"2022-01-18T14:08:17.572472","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 交叉验证气切分方法：https://www.kaggle.com/julian3833/reef-a-cv-strategy-subsequences/notebook\ndf_fold = pd.read_csv(\"../input/trainfold/train-5folds.csv\")\ndf = df.merge(df_fold[[\"image_id\",\"fold\"]],\"left\",on=\"image_id\")","metadata":{"papermill":{"duration":0.800881,"end_time":"2022-01-18T14:08:18.477278","exception":false,"start_time":"2022-01-18T14:08:17.676397","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ⭕ BBox Distribution","metadata":{"papermill":{"duration":0.046173,"end_time":"2022-01-18T14:08:18.570460","exception":false,"start_time":"2022-01-18T14:08:18.524287","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# bbox信息表，可以做统计eda查看框的分布\nbbox_df = pd.DataFrame(np.concatenate([bboxes_info, all_bboxes], axis=1),\n             columns=['image_id','video_id','sequence','xmid','ymid','w','h'])\nbbox_df[['xmid','ymid','w','h']] = bbox_df[['xmid','ymid','w','h']].astype(float)\nbbox_df['area'] = bbox_df.w * bbox_df.h * 1280 * 720\nbbox_df = bbox_df.merge(df[['image_id','fold']], on='image_id', how='left')\nbbox_df.head(2)","metadata":{"papermill":{"duration":0.184845,"end_time":"2022-01-18T14:08:18.802796","exception":false,"start_time":"2022-01-18T14:08:18.617951","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🍚 Dataset","metadata":{"papermill":{"duration":0.266252,"end_time":"2022-01-18T14:08:29.326285","exception":false,"start_time":"2022-01-18T14:08:29.060033","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 切分出数据集\ntrain_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\ntrain_files += list(train_df.image_path.unique())\nval_files += list(valid_df.image_path.unique())\nlen(train_files), len(val_files)","metadata":{"papermill":{"duration":0.284034,"end_time":"2022-01-18T14:08:29.870217","exception":false,"start_time":"2022-01-18T14:08:29.586183","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ⚙️ Configuration\nThe dataset config file requires\n1. The dataset root directory path and relative paths to `train / val / test` image directories (or *.txt files with image paths)\n2. The number of classes `nc` and \n3. A list of class `names`:`['cots']`","metadata":{"papermill":{"duration":0.258068,"end_time":"2022-01-18T14:08:30.386728","exception":false,"start_time":"2022-01-18T14:08:30.128660","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 构造yolov5的gbr.yaml（数据集配置文件）\n\nimport yaml\n\ncwd = '/kaggle/working/'\n# 写入训练集的图片路径\nwith open(os.path.join( cwd , 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path+'\\n')\n\n# 写入验证集的图片路径            \nwith open(os.path.join(cwd , 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path+'\\n')\n\n# yaml文件主体\ndata = dict(\n    path  = '/kaggle/working',\n    train =  os.path.join( cwd , 'train.txt') ,\n    val   =  os.path.join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'gbr.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.279834,"end_time":"2022-01-18T14:08:30.925473","exception":false,"start_time":"2022-01-18T14:08:30.645639","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 构造yolov5的hyp.yaml（超参数配置文件），包含学习率、loss参数、调度器、数据增强等等 ####","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/hyp.yaml\nlr0: 0.001  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.3  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.10  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.25  # image mosaic (probability)\nmixup: 0.25 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.27832,"end_time":"2022-01-18T14:08:31.464381","exception":false,"start_time":"2022-01-18T14:08:31.186061","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/custom_yolov5s6.yaml\n# Parameters\nnc: 1  # number of classes\ndepth_multiple: 0.33  # model depth multiple\nwidth_multiple: 0.50  # layer channel multiple\nanchors:\n  - [19,27,  44,40,  38,94]  # P3/8\n  - [96,68,  86,152,  180,137]  # P4/16\n  - [140,301,  303,264,  238,542]  # P5/32\n  - [436,615,  739,380,  925,792]  # P6/64\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 1]],  # 1-P2/4 # 1module\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [768, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [768]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 9-P6/64\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 11\n  ]\n\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [768, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 8], 1, Concat, [1]],  # cat backbone P5\n   [-1, 3, C3, [768, False]],  # 15\n\n   [-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 19\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 23 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 20], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 26 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 16], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [768, False]],  # 29 (P5/32-large)\n\n   [-1, 1, Conv, [768, 3, 2]],\n   [[-1, 12], 1, Concat, [1]],  # cat head P6\n   [-1, 3, C3, [1024, False]],  # 32 (P6/64-xlarge)\n\n   [[23, 26, 29, 32], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5, P6)\n  ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/custom_yolov5l6.yaml\n# Parameters\nnc: 1  # number of classes\ndepth_multiple: 1.0  # model depth multiple\nwidth_multiple: 1.0  # layer channel multiple\nanchors:\n  - [19,27,  44,40,  38,94]  # P3/8\n  - [96,68,  86,152,  180,137]  # P4/16\n  - [140,301,  303,264,  238,542]  # P5/32\n  - [436,615,  739,380,  925,792]  # P6/64\n\n# YOLOv5 v6.0 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n   [-1, 1, Conv, [128, 3, 1]],  # 1-P2/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n   [-1, 6, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [768, 3, 2]],  # 7-P5/32\n   [-1, 3, C3, [768]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 9-P6/64\n   [-1, 3, C3, [1024]],\n   [-1, 1, SPPF, [1024, 5]],  # 11\n  ]\n\n# YOLOv5 v6.0 head\nhead:\n  [[-1, 1, Conv, [768, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 8], 1, Concat, [1]],  # cat backbone P5\n   [-1, 3, C3, [768, False]],  # 15\n\n   [-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 19\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 23 (P3/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 20], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 26 (P4/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 16], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [768, False]],  # 29 (P5/32-large)\n\n   [-1, 1, Conv, [768, 3, 2]],\n   [[-1, 12], 1, Concat, [1]],  # cat head P6\n   [-1, 3, C3, [1024, False]],  # 32 (P6/64-xlarge)\n\n   [[23, 26, 29, 32], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5, P6)\n  ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 安装yolov5\n%cd /kaggle/working\n!rm -r /kaggle/working/yolov5\n# !git clone https://github.com/ultralytics/yolov5 # clone\n!cp -r /kaggle/input/yolov5-lib-ds /kaggle/working/yolov5\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # check","metadata":{"papermill":{"duration":12.434951,"end_time":"2022-01-18T14:08:44.700491","exception":false,"start_time":"2022-01-18T14:08:32.265540","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 🚅 Training","metadata":{"papermill":{"duration":0.260128,"end_time":"2022-01-18T14:08:45.218782","exception":false,"start_time":"2022-01-18T14:08:44.958654","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 开始训练\n!python train.py --img {DIM}\\\n--batch {BATCH}\\\n--epochs {EPOCHS}\\\n--optimizer {OPTMIZER}\\\n--data /kaggle/working/gbr.yaml\\\n--hyp /kaggle/working/hyp.yaml\\\n--cfg /kaggle/working/custom_yolov5l6.yaml\\\n--weights \"\"\\\n--project {PROJECT} --name {NAME}\\\n--exist-ok\n\n#--weights /kaggle/working/custom_yolov5s6.yaml\\","metadata":{"_kg_hide-output":true,"papermill":{"duration":12210.535534,"end_time":"2022-01-18T17:32:16.013809","exception":false,"start_time":"2022-01-18T14:08:45.478275","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}